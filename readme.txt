算法特定参数 (调优重点)
这些参数控制着 UFT（不确定性）和 FedIGMS（相似度）的组合行为。

1. UFT（不确定性税）相关参数
这两个参数在客户端（本地训练损失）和服务器（聚合权重）两方面惩罚不确定性。

--lambda_uft (默认: 0.1)

含义: 这是 UFT 在客户端本地损失函数中的权重。

公式: L_total = L_task + lambda_uft * R_UFT。

调优:

0: 完全禁用客户端的 UFT 正则化损失。服务器的聚合权重仍然会使用不确定性税 tau。

0.01 - 0.1: 较小的值。温和地引导客户端在训练时就关注不确定性，使其熵向全局平均值 U_bar 靠拢。（推荐的起始范围）

1.0 或更高: 较大的值。强力迫使所有客户端产生相似的不确定性。如果设得太高，可能会损害主要任务（如 Dice 分数）的性能。

--uft_beta_u (默认: 1.0)

含义: UFT "税率" (tau) 的敏感度系数。

公式: tau_i = exp(beta_u * U_i)。这个 tau_i 稍后会用于服务器聚合权重中（w_i ∝ 1 / (1 + tau_i)）。

调优:

beta_u 越大，不确定性 U_i（熵）的微小差异就会导致 tau_i 税率的巨大差异，从而在聚合时对高不确定性客户端施加更重的惩罚。

beta_u 越小（例如 0.1），所有客户端的税率 tau_i 都会很接近 exp(0) 即 1，使得不确定性税基本失效。

建议: 保持 1.0 作为默认值。如果您发现所有客户端的熵（Ue）在日志中都非常接近，您可以适当提高此值以放大差异。如果某些客户端的熵极高导致其权重过低，可以适当降低此值。

2. FedIGMS-Mixer（混合相似度）相关参数
这些参数控制服务器如何计算客户端之间的特征相似度（S_mix = alpha*RAD + (1-alpha)*MINE）。

--alpha_init (默认: 0.5)

含义: 混合相似度中 alpha 的初始值。alpha 控制 RAD（几何相似度）和 MINE（互信息相似度）的平衡。

调优:

0.5: 最中立的开始，假设 RAD 和 MINE 同样重要。

1.0: 初始时 100% 信任 RAD。

0.0: 初始时 100% 信任 MINE。

建议: 保持 0.5。这个算法的亮点在于它会自动学习 alpha。new_main.py 中的 update_alpha 函数会根据验证集（Validation Set）的性能反馈，自动调整 alpha（日志中会打印 Current Alpha），使其偏向能带来更大性能提升的那个相似度度量。

--rad_gamma (默认: 1.0)

含义: RAD 相似度的"锐度"系数。

公式: Sim_RAD = exp(-gamma * Dist_RAD)。

调优:

gamma 越大，相似度对距离越敏感。微小的距离差异也会导致相似度急剧下降（趋近于0）。

gamma 越小（例如 0.1），相似度曲线越平缓，所有客户端的相似度都可能很高（趋近于1）。

建议: 保持 1.0。这是一个用于缩放距离的尺度参数，通常不需要首先调整。

--mine_hidden (默认: 128) 和 --lr_mine (默认: 1e-3)

含义: 这两个参数用于配置服务器上那个用来估计 MINE（互信息）的小型神经网络（SmallCritic）。

调优:

mine_hidden: MINE 估计器的隐藏层大小。

lr_mine: MINE 估计器的学习率。

建议: 通常不需要修改。128 和 1e-3 对于这个辅助任务来说是稳定且足够的。

3. 算法控制参数
--sim_start_round (默认: 5)

含义: 在第几轮开始启用相似度聚合。在此轮数之前，算法将使用标准的 FedAvg（或按样本量加权平均）。

调优:

原因: 在训练的最初几轮，模型的特征（z 和 shadow）是随机且无意义的。基于这些特征计算相似度（RAD 或 MINE）不仅不准确，而且可能有害。

建议: 保持 5 或 10。这给了模型几轮 "热身" 时间，使其特征变得有意义之后，再开始根据特征相似度进行加权聚合。

标准训练参数 (常规调优)
这些是机器学习中常见的参数，它们同样重要。

--lr (默认: 1e-3)

含义: 客户端本地训练的学习率。

调优: 这是最重要的参数之一。1e-3 (0.001) 或 1e-4 (0.0001) 是 Adam 优化器在分割任务中的常用值。如果损失下降太快或发散，请降低学习率。

--epochs (默认: 1)

含义: 每个客户端在每一轮通信中本地训练的轮数。

调优:

1: 客户端 "浅度" 训练，更接近全局模型，但通信开销大。

5 或 10: 客户端 "深度" 训练，可以更快收敛，但也更容易导致 "客户端漂移"（Client Drift），即本地模型偏离全局模型太远。

建议: 从 1 开始。如果收敛太慢，可以尝试增加到 3 或 5。

--batch-size (默认: 8)

含义: 批量大小。

调优: 在显存允许的情况下，越大越好（例如 16 或 32），有助于更稳定地估计梯度和熵（U_i）。

--dropout_rate (默认: 0.2)

含义: UNet_pro 中 Dropout 层的丢弃率，用于防止过拟合。

调优: 0.1 到 0.3 都是合理范围。如果模型在训练集上表现很好但在验证/测试集上表现很差（过拟合），可以适当提高此值（例如到 0.3）。

总结：调优策略
起点: 使用脚本中提供的默认值开始。

首要关注 (客户端):

调整标准参数，特别是 --lr（例如 1e-3 或 1e-4），确保模型能正常收敛（L_task 在下降）。

调整 --lambda_uft。这是影响客户端行为最直接的参数。观察 R_UFT 损失项，确保它不会压倒 L_task。

观察 (服务器):

观察日志中 Current Alpha 的变化。看它是偏向 1.0 (RAD) 还是 0.0 (MINE)。这由算法自动完成。

观察日志中 Final Aggr Weights。查看权重是否在不同客户端之间产生了有意义的区分。

次要调整 (服务器):

如果 Final Aggr Weights 总是趋近于均匀分布，可能是 tau 税率或 w_sim 区分度不够。

如果 tau_tax 都很接近（例如都是 1.0），可以尝试提高 --uft_beta_u（例如到 2.0）来放大熵的差异。

如果 w_sim 都很接近，可能是 MINE/RAD 认为所有客户端都"一样"。这通常不需要担心，因为 alpha 会自动调整。